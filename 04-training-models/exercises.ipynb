{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Which Linear Regression training algorithm can you use if you have a training set with millions of features\n",
    "Batch gradient descent, gradient descent, mini-batch gradient descent\n",
    "\n",
    "### 2. Suppose the features in your training set have very different scales. Which algorithm might suffer from this, and how? What can you do about it?\n",
    "Gradient descent, because it will converge slower. I can scale features using MinMaxScaler or StandardScaler. Regularized models can end up in non-optimal solution, because they could ignore features with small values.\n",
    "\n",
    "### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "No, because logistic regression optimization problem is convex\n",
    "\n",
    "### 4. Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?\n",
    "Nearly, yes\n",
    "\n",
    "### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "If the validation error consistently goes up after every epoch, then one posibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However if the training error is not going up, then your model is overfitting the training set and you should stop training.\n",
    "\n",
    "### 6. Is it a good idea to stop Mini-batch Gradient Descent immidiately when the validation error goes up?\n",
    "Not really, because in mini-batch gradient descent is pretty common that the validation errors is going up and down in single steps, while model is actually converging.\n",
    "\n",
    "### 7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastests? Which will actually converge? How can you make the others converge as well?\n",
    "The fastest is stochastic gradient descent, because it is using single point in each step\n",
    "Batch gradient descent converges without any improvements. To make mini-batch and stochastic ones converges we should reduce learning rate in further epochs.\n",
    "\n",
    "### 8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are tree ways to solve this?\n",
    "The model is overfitting the training data. Solutions: gather more training data, reduce polynomial degree, regularize the model.\n",
    "\n",
    "### 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the models suffers from high bias or high variance? Should you increase the regularization hyperparameter :alfa: or reduce it?\n",
    "The model is suffering from high bias. To fix this we should reduce regularization parameter.\n",
    "\n",
    "### 10. Why would you want to use:\n",
    "a. Ridge Regression instead of plain Linear Regresion (i.e., without any regularization)?\n",
    "You should always use some kind of regularization\n",
    "b. Lasso instead of Ridge Regression?\n",
    "When some of the features might be useless\n",
    "c. Elastic Net instead of Lasso?\n",
    "\n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression (without using Scikit-Learn)\n",
    "Two Logistic Regression, because classes are not exclusive.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)]\n",
    "y = iris['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def with_bias(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "X_with_bias = with_bias(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(test_ratio * total_size)\n",
    "validation_size = int(validation_ratio * test_size)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = len(np.unique(y))\n",
    "    m = len(y)\n",
    "    y_one_hot = np.zeros((m, n_classes))\n",
    "    y_one_hot[np.arange(m), y] = 1\n",
    "    return y_one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.]])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "y_one_hot = to_one_hot(y)\n",
    "\n",
    "# theta = np.zeros((y_one_hot.shape[1], X_with_bias.shape[1]))\n",
    "#\n",
    "# print(y.shape)\n",
    "# print(np.unique(y))\n",
    "#\n",
    "# m = y.size\n",
    "# sk = np.dot(X, np.transpose(theta))\n",
    "# exp_sk = np.exp(sk)\n",
    "# pk = exp_sk / np.sum(exp_sk, 0)\n",
    "# J = - np.sum(y_one_hot * np.log(pk)) / m\n",
    "# delta_J = np.dot(np.transpose(pk - y_one_hot), X) / m\n",
    "eta = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# def predict(X):\n",
    "#     sk = np.dot(X, np.transpose(theta))\n",
    "#     exp_sk = np.exp(sk)\n",
    "#     pk = exp_sk / np.sum(exp_sk, 1)\n",
    "#     return np.argmax(pk, 1)\n",
    "#\n",
    "# def predict_proba(X):\n",
    "#     sk = np.dot(X, np.transpose(theta))\n",
    "#     exp_sk = np.exp(sk)\n",
    "#     pk = exp_sk / np.sum(exp_sk, 1)\n",
    "#     return pk\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    def __init__(self, eta=0.01, max_iterations=100, patience=3):\n",
    "        self.eta = eta\n",
    "        self.max_iterations = max_iterations\n",
    "        self.patience = patience\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        y_one_hot = to_one_hot(y)\n",
    "\n",
    "        # theta = np.zeros((classes.shape[0], X.shape[1]))\n",
    "        theta = np.random.randn(classes.shape[0], X.shape[1])\n",
    "\n",
    "        best_J = sys.maxsize\n",
    "        best_theta = theta\n",
    "        num_of_iterations_without_improve = 0\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        for epoch in range(self.max_iterations):\n",
    "            m = y.size\n",
    "            sk = np.dot(X, np.transpose(theta))\n",
    "            exp_sk = np.exp(sk)\n",
    "            exp_sum = np.sum(exp_sk, axis=1, keepdims=True)\n",
    "            pk = exp_sk / exp_sum\n",
    "            delta_J = np.dot(np.transpose(pk - y_one_hot), X) / m\n",
    "            theta = theta - self.eta * delta_J\n",
    "            J = - np.sum(y_one_hot * np.log(pk + epsilon)) / m\n",
    "            if J < best_J:\n",
    "                num_of_iterations_without_improve = 0\n",
    "                best_theta = theta\n",
    "                best_J = J\n",
    "            elif num_of_iterations_without_improve >= self.patience:\n",
    "                print(self.patience, \"iterations without loss improvement\")\n",
    "                print(\"Returning\", best_theta, \"as final model parameters, which results in loss equal to\", best_J)\n",
    "                break\n",
    "            else:\n",
    "                num_of_iterations_without_improve += 1\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"Epoch:\", epoch, \"loss:\", J)\n",
    "\n",
    "        self.theta = best_theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Theta shape:\", self.theta.shape)\n",
    "        print(\"X shape:\", X.shape)\n",
    "        sk = np.dot(X, np.transpose(self.theta))\n",
    "        print(\"SK shape:\", sk.shape)\n",
    "        exp_sk = np.exp(sk)\n",
    "        exp_sum = np.sum(exp_sk, axis=1, keepdims=True)\n",
    "        print(\"Sum shape:\", exp_sum.shape)\n",
    "        pk = exp_sk / exp_sum\n",
    "        return np.argmax(pk, 1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 2.362342730571486\n",
      "Epoch: 500 loss: 0.3901456732957007\n",
      "Epoch: 1000 loss: 0.3006303713791963\n",
      "Epoch: 1500 loss: 0.2557342432944118\n",
      "Epoch: 2000 loss: 0.2268602426614136\n",
      "Epoch: 2500 loss: 0.20622628704901602\n",
      "Epoch: 3000 loss: 0.19056359354600286\n",
      "Epoch: 3500 loss: 0.17818617431018688\n",
      "Epoch: 4000 loss: 0.16811391527990094\n",
      "Epoch: 4500 loss: 0.15973039860713542\n",
      "Epoch: 5000 loss: 0.15262549740322198\n",
      "Epoch: 5500 loss: 0.14651442988232588\n",
      "Epoch: 6000 loss: 0.1411926471237409\n",
      "Epoch: 6500 loss: 0.13650911119871476\n",
      "Epoch: 7000 loss: 0.13234968767190888\n",
      "Epoch: 7500 loss: 0.12862641311230055\n",
      "Epoch: 8000 loss: 0.12527033001867477\n",
      "Epoch: 8500 loss: 0.12222656943078447\n",
      "Epoch: 9000 loss: 0.11945089477253538\n",
      "Epoch: 9500 loss: 0.11690722163282337\n",
      "Epoch: 10000 loss: 0.11456580489790852\n",
      "Epoch: 10500 loss: 0.11240189180671357\n",
      "Epoch: 11000 loss: 0.11039470637518366\n",
      "Epoch: 11500 loss: 0.10852667344114651\n",
      "Epoch: 12000 loss: 0.10678281860424053\n",
      "Epoch: 12500 loss: 0.10515029905569177\n",
      "Epoch: 13000 loss: 0.10361803302865519\n",
      "Epoch: 13500 loss: 0.10217640440935628\n",
      "Epoch: 14000 loss: 0.10081702523588922\n",
      "Epoch: 14500 loss: 0.09953254321700332\n",
      "Epoch: 15000 loss: 0.09831648458080136\n",
      "Epoch: 15500 loss: 0.09716312488255101\n",
      "Epoch: 16000 loss: 0.09606738211235108\n",
      "Epoch: 16500 loss: 0.09502472771945268\n",
      "Epoch: 17000 loss: 0.09403111213057146\n",
      "Epoch: 17500 loss: 0.09308290206904749\n",
      "Epoch: 18000 loss: 0.09217682754046211\n",
      "Epoch: 18500 loss: 0.09130993678166698\n",
      "Epoch: 19000 loss: 0.09047955780565867\n",
      "Epoch: 19500 loss: 0.08968326543748847\n",
      "Epoch: 20000 loss: 0.08891885294356545\n",
      "Epoch: 20500 loss: 0.08818430752108182\n",
      "Epoch: 21000 loss: 0.0874777890454805\n",
      "Epoch: 21500 loss: 0.08679761157919386\n",
      "Epoch: 22000 loss: 0.08614222722986409\n",
      "Epoch: 22500 loss: 0.08551021201518881\n",
      "Epoch: 23000 loss: 0.08490025344772899\n",
      "Epoch: 23500 loss: 0.08431113959902758\n",
      "Epoch: 24000 loss: 0.08374174944024039\n",
      "Epoch: 24500 loss: 0.08319104428774671\n",
      "Epoch: 25000 loss: 0.08265806020814484\n",
      "Epoch: 25500 loss: 0.08214190125862776\n",
      "Epoch: 26000 loss: 0.08164173345678973\n",
      "Epoch: 26500 loss: 0.08115677938905005\n",
      "Epoch: 27000 loss: 0.08068631337962563\n",
      "Epoch: 27500 loss: 0.08022965715274398\n",
      "Epoch: 28000 loss: 0.07978617592990866\n",
      "Epoch: 28500 loss: 0.07935527491177034\n",
      "Epoch: 29000 loss: 0.07893639610076705\n",
      "Epoch: 29500 loss: 0.07852901542633617\n",
      "Epoch: 30000 loss: 0.07813264013933999\n",
      "Epoch: 30500 loss: 0.07774680644650453\n",
      "Epoch: 31000 loss: 0.07737107735924977\n",
      "Epoch: 31500 loss: 0.07700504073438715\n",
      "Epoch: 32000 loss: 0.07664830748683682\n",
      "Epoch: 32500 loss: 0.07630050995683886\n",
      "Epoch: 33000 loss: 0.0759613004161589\n",
      "Epoch: 33500 loss: 0.07563034969954709\n",
      "Epoch: 34000 loss: 0.07530734594924988\n",
      "Epoch: 34500 loss: 0.07499199346172197\n",
      "Epoch: 35000 loss: 0.07468401162686633\n",
      "Epoch: 35500 loss: 0.0743831339511677\n",
      "Epoch: 36000 loss: 0.07408910715700301\n",
      "Epoch: 36500 loss: 0.07380169035121263\n",
      "Epoch: 37000 loss: 0.073520654256734\n",
      "Epoch: 37500 loss: 0.0732457805017264\n",
      "Epoch: 38000 loss: 0.07297686096117646\n",
      "Epoch: 38500 loss: 0.07271369714646858\n",
      "Epoch: 39000 loss: 0.07245609963884837\n",
      "Epoch: 39500 loss: 0.07220388756309899\n",
      "Epoch: 40000 loss: 0.07195688809810041\n",
      "Epoch: 40500 loss: 0.07171493602125806\n",
      "Epoch: 41000 loss: 0.07147787328406381\n",
      "Epoch: 41500 loss: 0.07124554861630863\n",
      "Epoch: 42000 loss: 0.07101781715668615\n",
      "Epoch: 42500 loss: 0.07079454010773441\n",
      "Epoch: 43000 loss: 0.07057558441324106\n",
      "Epoch: 43500 loss: 0.07036082245640495\n",
      "Epoch: 44000 loss: 0.07015013177719212\n",
      "Epoch: 44500 loss: 0.06994339480746065\n",
      "Epoch: 45000 loss: 0.06974049862254762\n",
      "Epoch: 45500 loss: 0.06954133470812242\n",
      "Epoch: 46000 loss: 0.06934579874120896\n",
      "Epoch: 46500 loss: 0.06915379038436935\n",
      "Epoch: 47000 loss: 0.06896521309212407\n",
      "Epoch: 47500 loss: 0.06877997392875647\n",
      "Epoch: 48000 loss: 0.06859798339671917\n",
      "Epoch: 48500 loss: 0.06841915527491918\n",
      "Epoch: 49000 loss: 0.06824340646621685\n",
      "Epoch: 49500 loss: 0.068070656853523\n",
      "Epoch: 50000 loss: 0.06790082916392726\n",
      "Theta shape: (3, 3)\n",
      "X shape: (1, 3)\n",
      "SK shape: (1, 3)\n",
      "Sum shape: (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([2])"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_reg =  SoftmaxRegression(eta=0.1, max_iterations=50001)\n",
    "soft_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "X = np.array([[5, 2]])\n",
    "soft_reg.predict(with_bias(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape: (3, 3)\n",
      "X shape: (6, 3)\n",
      "SK shape: (6, 3)\n",
      "Sum shape: (6, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8333333333333334"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_predict = soft_reg.predict(X_valid)\n",
    "\n",
    "np.mean(y_valid_predict == y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape: (3, 3)\n",
      "X shape: (30, 3)\n",
      "SK shape: (30, 3)\n",
      "Sum shape: (30, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9333333333333333"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = soft_reg.predict(X_test)\n",
    "\n",
    "np.mean(y_test_predict == y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "y_valid_one_hot = to_one_hot(y_valid)\n",
    "\n",
    "# number_of_classes x number of features\n",
    "theta = np.zeros((y_one_hot.shape[1], X_valid.shape[1]))\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(np.unique(y))\n",
    "\n",
    "\n",
    "m = y.size\n",
    "\n",
    "# number of examples x number of classes\n",
    "sk = np.dot(X_valid, np.transpose(theta))\n",
    "exp_sk = np.exp(sk)\n",
    "\n",
    "# number of examples x number of classes\n",
    "sum_exp_sk = np.sum(exp_sk, axis=1, keepdims=True)\n",
    "\n",
    "# number of examples x number of classes\n",
    "pk = exp_sk / sum_exp_sk\n",
    "J = - np.sum(y_valid_one_hot * np.log(pk)) / m\n",
    "delta_J = np.dot(np.transpose(pk - y_valid_one_hot), X_valid) / m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}